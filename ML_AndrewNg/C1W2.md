# Course 1 Week 2

## Multiple linear regression

### Multiple features (variables)

#### Notation:
$x_j = j^{th}$ feature  
$n$ = number of features  
$\overrightarrow{x}^{(i)}$ = features of $i^{th}$ training example  
$x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example

#### Model:
Previously: $f_{w,b}(x) = wx + b$  
Now: $f_{w,b}(x) = w_1x_1 +w_2x_2+w_3x_3 ... + b$  
$\overrightarrow{w} = [w_1,w_2,w_3, ... w_n]$  
$\overrightarrow{x} = [x_1,x_2,x_3, ... x_n]$  
$f_{\overrightarrow{w},b}(\overrightarrow{x}) = \overrightarrow{w}·\overrightarrow{x} + b$ (Note: · is dot product)

### Vectorization
$\overrightarrow{w},\overrightarrow{x}, b$  
Linear algebra: count from 1
Code: count from 0
Python (NumPy):
```
w = np.array([1.0,2.5,-3.3])
x = np.array([10,20,30])
b = 4
```
$w_1$ -> `w[0]`
#### Implementation
Vectorization $f_{\overrightarrow{w},b}(\overrightarrow{x}) = \overrightarrow{w}·\overrightarrow{x} + b$:
`f = np.dot(w,x) + b` (faster)
is equal to:
```
f = 0
for j in range(0,n):
    f = f+w[j]*x[j]
f += b
```
#### Gradient Descent Implementation
Derivative: $\overrightarrow{d} = [d_1,d_2,d_3, ... d_n]$  
$\overrightarrow{w} = \overrightarrow{w} - 0.1\overrightarrow{d} $  
With vectorization:
`w = np.array([...])`
`d = np.array([...])`
`w = w - 0.1*d`