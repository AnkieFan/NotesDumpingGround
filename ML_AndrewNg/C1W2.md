# Course 1 Week 2

## Multiple linear regression

### Multiple features (variables)
#### Notation:
$x_j = j^{th}$ feature  
$n$ = number of features  
$\overrightarrow{x}^{(i)}$ = features of $i^{th}$ training example  
$x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example

#### Model:
Previously: $f_{w,b}(x) = wx + b$  
Now: $f_{w,b}(x) = w_1x_1 +w_2x_2+w_3x_3 ... + b$  
$\overrightarrow{w} = [w_1,w_2,w_3, ... w_n]$  
$\overrightarrow{x} = [x_1,x_2,x_3, ... x_n]$  
$f_{\overrightarrow{w},b}(\overrightarrow{x}) = \overrightarrow{w}·\overrightarrow{x} + b$ (Note: · is dot product)

### Vectorization
$\overrightarrow{w},\overrightarrow{x}, b$  
Linear algebra: count from 1
Code: count from 0
Python (NumPy):
```
w = np.array([1.0,2.5,-3.3])
x = np.array([10,20,30])
b = 4
```
$w_1$ -> `w[0]`  

#### Implementation
Vectorization $f_{\overrightarrow{w},b}(\overrightarrow{x}) = \overrightarrow{w}·\overrightarrow{x} + b$:
`f = np.dot(w,x) + b` (faster)
*More in Code in Python part*

#### Gradient Descent Implementation
Derivative: $\overrightarrow{d} = [d_1,d_2,d_3, ... d_n]$  
$\overrightarrow{w} = \overrightarrow{w} - 0.1\overrightarrow{d} $  
With vectorization:
`w = np.array([...])`
`d = np.array([...])`
`w = w - 0.1*d`

### Disadvantages of gradient descent
+ Doesn't generalize to other learning algorithms
+ Slow ehn number of features is large
+ **Normal equation** only for linear regression

## Gradient Descent in practice

### Feature Scaling
To make Gradient Descent run faster
+ the feature range is large -> weight should be smaller normally

e.g.:
$300\leq x_1 \leq 2000$
$x_{1,scaled}=\frac{x_1}{2000}$
$0.15 \leq x_{1,scaled} \leq 1$

### Mean Normalization
e.g.:
$300\leq x_1 \leq 2000$
Calculate average of $x_1$ : $\mu_1 = 600$
$x_{1,scaled}=\frac{x_1-\mu_1}{2000-300}$ (max-min)
$-0.18 \leq x_{1,scaled} \leq 0.82$

### Z-score normalization
Z-score: standard deviation $\sigma$
$300\leq x_1 \leq 2000$
Calculate average of $x_1$ : $\mu_1 = 600$
Calculate standard deviation of $x_1$ : $\sigma_1 = 450$
$x_{1,scaled}=\frac{x_1-\mu_1}{\sigma_1}$ (max-min)
$-0.67 \leq x_{1,scaled} \leq 3.1$

### Checking gradient descent for convergence
+ $J_{(w, b)}$ should decrease after every iteration
+ $J_{(w, b)}$ likely converged by n iterations
+ e.g.: Convergence test:
  + Let $\epsilon$ be $10^{-3}$
  + if $J_{(w, b)}$ decreases by $\leq\epsilon$ in one iteration -> **declare convergence**
  
### Choosing the learning rate
If $J_{(w, b)}$ sometimes goes up and sometimes goes down: bug or learning rate is too large
+ try $\alpha$ like: 0.0001, 0.01, 0.1, 1

### Polynomial regression


## Code in python:

### NumPy
`import numpy as np`  
#### Fill arrays with value
`a = np.zeros(4)`  a = [0. 0. 0. 0.] (float64) a.shape = (4,)  
`a = np.zero((4,))`  a = [0. 0. 0. 0.] (float64)  
`a = np.random.random_sample(4)` a = [0.sasda 0.sfasfs 0.rewws 0.ajdfakd] (float64)  
#### Fill arrays with value but do not accept shape as input argument
`a = np.arange(4.)` a = [0. 1. 2. 3.] (float64)  
`a = np.random.rand(4)`  a = [0.sasda 0.sfasfs 0.rewws 0.ajdfakd] (float64)  
#### Fill with user specified values
`a = np.array([5,4,3,2])`  a = [5 4 3 2] (int64)  
`a = np.array([5.,4,3,2])` a = [5. 4. 3. 2.] (float64)  
#### Indexing, Slicing, Single vector operations to an np.array is same to normal one
`a = np.arange(10)`  
`a` = [0 1 2 3 4 5 6 7 8 9], `a[2]` = 2, `a.shape` = (10,), `len(a)` = 10  
`a[2:7:1]` = [2 3 4 5 6]  
`a[3:]` = [3 4 5 6 7 8 9]  
`a[:3]` =  [0 1 2]  
`a[:]` = [0 1 2 3 4 5 6 7 8 9]  
`np.sum(a)` = 45  
`np.mean(a)` = 4.5  
`a**2` = [ 0  1  4  9 16 25 36 49 64 81]  
#### Vector vector element wise operations:
```
a = np.array([ 1, 2, 3, 4])
b = np.array([-1,-2, 3, 4])
c = np.array([1, 2])
```
`a + b` = [0 0 6 8]  
`a + c`: error  
#### Scalar Vector operations
`a = np.array([1, 2, 3, 4])`  
`5*a` = [ 5 10 15 20]  
#### Vector Vector dot product
```
for i in range(a.shape[0]):
    x = x + a[i] * b[i]
```
is slower than:  
`np.dot(a,b)`  
#### Matrix creation
`a = np.zeros((1,5))` `a.shape` = (1, 5), `a` = [\[0. 0. 0. 0. 0.]]  
`a = np.zeros((2,1))` `a.shape` = (2, 1), `a` = [\[0.]
[0.]] (两行一列，有换行)  
`a = np.random.random_sample((1, 1))` `a.shape` = (1,1), `a` = [\[0.xsdasdqw]]  
#### Operations on Matrices
`a = np.arange(6).reshape(-1, 2)`  (means 2 columns)  
is equal to
`a = np.arange(6).reshape(3, -1)`  
is equal to
`a = np.arange(6).reshape(3, 2)`  
a.shape: (3, 2), 
a= [[0 1]
 [2 3]
 [4 5]]
##### Slicing
`a = np.arange(20).reshape(-1, 10)`  
a = 
[\[ 0  1  2  3  4  5  6  7  8  9]
 [10 11 12 13 14 15 16 17 18 19]]
`a[0, 2:7:1]` = [2 3 4 5 6]  
`a[:, 2:7:1]` =  
 [\[ 2  3  4  5  6]  
 [12 13 14 15 16]]  
 `a[:,:]` =  
 [\[ 0  1  2  3  4  5  6  7  8  9]  
 [10 11 12 13 14 15 16 17 18 19]]  
 `a[1,:]` =  [10 11 12 13 14 15 16 17 18 19]  
`a[1]`   =  [10 11 12 13 14 15 16 17 18 19]  
`a[1,:] == a[1]`: [ True  True  True  True  True  True  True  True  True  True]