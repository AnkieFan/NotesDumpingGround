# Course 2 Week 4: Decision Tree

+ Decision 1: How to choose what feature to split on at each node?
  + Maximize Purity (or minimize impurity)
+ Decision 2: When do you stop splitting?
  + When a node is 100% one class
  + When splitting a node will result in the tree exceeding a maximum depth

## Measuring purity/impurity
### Entropy function
$p_1 =$ fraction of examples that are cats
$p_0 = 1 - p_1$
$H(p_1) = -p_1 \log_2(p_1)-p_0\log_2(p_0) = -p_1\log_2(p_1) - (1 - p_1)\log_2(1 - p_1)$

### Information gain
Choosing a split: which split is better?
E.g.: Ear shape (10 animals):
+ At root node: $p_1^{root} = \frac{5}{10} = 0.5$, $H(0.5) = 1$ 
+ Pointy: 5 and 4 are cats: $p_1^{left} = \frac{4}{5} = 0.8$,$w_1^{left} = \frac{5}{10}$, $H(0.8) = 0.72$
+ Floppy: 5 and 1 are cats: $p_1^{right} = \frac{1}{5} = 0.2$,$w_1^{right} = \frac{5}{10}$, $H(0.2) = 0.72$
Information gain: $H(p_1^{root}) - (w^{left}H(p_1^{left}) + w^{right}H(p_1^{right}))$

## Multiple Features
One-hot encoding: use 1 or 0 to present True & False, so we can classify them
E.g.: color: red, yellow green
red|yellow|green
---|---|---
1|0|0
0|1|0
0|0|1

## Continuous features 
not discrete
E.g.: weight -> x-axis
Is cat -> y-axis
Find a threshold with higher information gain, use **regression** to build a tree

## Tree ensembles
Multiple decision trees: not rubust
Vote from some trees and semble them together

### Random forest algorithm
Bagged decision trees
#### Generating a tree sample:
Given training set of size m
+ For $b=1$ to $B$:
  + Use sampling with replacement to create a new training set of size $m$
  + Train a decision tree on the new dataset

#### Randomizing the feature choice
At each node, when choosing a feature to use to split, if $n$ features are avaliable. pick a random subset of $k<n$ features and allow the algorithm to only choose from that subset of features.

## XGBoost
Given training set of size m
+ For $b=1$ to $B$:
  + Use sampling with replacement to create a new training set of size $m$
    + But instead of picking from all examples with equal $(1/m)$ probability, make it more likely to pick misclassfied examples from previously trained trees. 只练之前不擅长的部分。
  + Train a decision tree on the new dataset

### XGBoost (eXtreme Gradient Boosting)
+ Open source implementation of boosted trees
+ Fast efficient implementation
+ Good choice of default splitting criteria and criteria for when to stop splitting
+ Build in regularization to prevent overfitting
+ Highly competitive algorithm for machine learning competitions

#### Implementation
+ Claasification:
```
from xgboost import XGBClassifier
model = XGBClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

+ Regression:
```
from xgboost import XGBRegressor
model = XGBRegressor()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

## Decision Tree vs Neural Networks:
### Decision Tree:
+ works well on tabular (structured) data
+ Not recommended for unstructured data(image, audio, text)
+ Fast
+ Small decision trees may be human interpretable
### Neural Networks:
+ Works well on all types of data
+ May be slower than a decision tree
+ Works with transfer learing
+ When building a system of mulyiple models working together, it might be easier to string together multiple neural networks