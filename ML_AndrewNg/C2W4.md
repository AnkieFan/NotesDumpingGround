# Course 2 Week 4: Decision Tree

+ Decision 1: How to choose what feature to split on at each node?
  + Maximize Purity (or minimize impurity)
+ Decision 2: When do you stop splitting?
  + When a node is 100% one class
  + When splitting a node will result in the tree exceeding a maximum depth

## Measuring purity/impurity
### Entropy function
$p_1 =$ fraction of examples that are cats
$p_0 = 1 - p_1$
$H(p_1) = -p_1 \log_2(p_1)-p_0\log_2(p_0) = -p_1\log_2(p_1) - (1 - p_1)\log_2(1 - p_1)$

### Information gain
Choosing a split: which split is better?
E.g.: Ear shape (10 animals):
+ At root node: $p_1^{root} = \frac{5}{10} = 0.5$, $H(0.5) = 1$ 
+ Pointy: 5 and 4 are cats: $p_1^{left} = \frac{4}{5} = 0.8$,$w_1^{left} = \frac{5}{10}$, $H(0.8) = 0.72$
+ Floppy: 5 and 1 are cats: $p_1^{right} = \frac{1}{5} = 0.2$,$w_1^{right} = \frac{5}{10}$, $H(0.2) = 0.72$
Information gain: $H(p_1^{root}) - (w^{left}H(p_1^{left}) + w^{right}H(p_1^{right}))$

## Multiple Features
One-hot encoding: use 1 or 0 to present True & False, so we can classify them
E.g.: color: red, yellow green
red|yellow|green
---|---|---
1|0|0
0|1|0
0|0|1

## Continuous features 
not discrete
E.g.: weight -> x-axis
Is cat -> y-axis
Find a threshold with higher information gain