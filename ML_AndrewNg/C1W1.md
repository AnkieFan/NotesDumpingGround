# Course 1 Week 1

## Machine Learning Algorithms
- Supervised Learning
- Unsupervised learning
- Recommender systems
- Reinforcement Learning 

## Supervised Learning
X     ->       Y
input -> output label
+ Learns from data **labeled** with the "right answers" (correct label)
+ X & Y mapping

Examples:
### Regression problem
Any results 
+ Predict a **number**
+ **Infinitely** many possible outputs


### Classification problem
Only certain possible results
+ Predict **categories** (Non-numerical)
+ Find boundaries]

## Unsupervised Learning
Find something interesting in **unlabeled** data
Find **structure** in the data

+ Clustering: Group similar data points together
+ Anomaly detection: Find unusual data points
+ Dimensionality reduction: Compress data using fewer numbers

## Regression Model

### Linear Regression
A supervised learning model

Notation:
*x* = 'input' variable / feature
*y* = 'output'/'target' variable
$\hat{y}$ (y-hat) = estimate/prediction for output
*m* = number of training examples
*(x, y)* = single training example

### Training process:
+ training set (features, target)
+ Learning Algorithm
+ *x* -> *f* (function/hypothesis) -> *y-hat*
+ feature         model            prediction
e.g.: house price calculation: size -> *f* -> price

### Linear Regression model
$f_{w,b}(x) = wx + b$  
$\hat{y} = f_{w,b}(x)$  
+ *w, b*: parameters/coefficients/weights
+ **Univariate**: one variable

### Cost function
Compare prediction $\hat{y}$ to the target ***y*** (Error)
e.g.: Squared error cost function (suitable for linear regression)
+ $J_{(w, b)} = \frac{1}{2m}\sum^m_{i=1}(f_{w,b}(x^{(i)})-y^{(i)})^2$
+ Find w, b to make $J_{(w, b)}$ small -> $minimize J_{(w, b)}$ 
+ Squared error cost function is always bowl shape -> always have only one local/global minimum

## Gradient Descent
$w = w - \alpha \frac{d}{d\omega}J(w,b)$  
$b = b - \alpha \frac{d}{d\omega}J(w,b)$  
$\alpha$: step size\learning rate
$w, b$ should be updated **simultaneously** (in the same time)

### Implementation:
```
temp_w = w - ...
temp_b = b - ...
w = temp_w
b = temp_b
```

### Disadvantage:
Will stuck in local minimum
Reason: local minimum -> $\frac{d}{d\omega}J(w,b) = 0$ w doesn't change anymore.

### Use Gradient Descent in Linear Regression:
1. compute gradient:
    $\frac{dJ(w,b)}{dw} = \frac{1}{m}\sum^{m-1}_{i=0}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)}$
    $\frac{dJ(w,b)}{db} = \frac{1}{m}\sum^{m-1}_{i=0}(f_{w,b}(x^{(i)})-y^{(i)})$
2. compute cost
3. gradient descent

## Code in python

### Lab: Model representation
Train set: `x_train = np.array([1.0, 2.0])`  

`.shape`: . Numpy arrays have a `.shape` parameter. `x_train.shape` returns a python tuple with an entry for each dimension. `x_train.shape[0]` is the length of the array. (or just `len(x_train)`)  

`scatter()` in `matploylib` library: plot the data points
