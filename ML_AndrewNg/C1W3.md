# Course 1 Week 3

## Logistic regression: classification
Sigmoid function/Logistic function
![](Img/sigmoid.png)
$z = f_{\overrightarrow{w},b}(\overrightarrow{x}) = \overrightarrow{w}·\overrightarrow{x} + b$  
$g(z) = \frac{1}{q + e^{-z}}$  
$0< g(z) < 1$  

---
$f_{\overrightarrow{w},b}(\overrightarrow{x}) = \frac{1}{q + e^{-(\overrightarrow{w}·\overrightarrow{x} + b)}}$
Since either 0 or 1 (True or False), Probability:
$f_{\overrightarrow{w},b}(\overrightarrow{x}) = P(y=1|\overrightarrow{x};\overrightarrow{w},b)$
Probability that y is 1, given input $\overrightarrow{x}$, parameters $\overrightarrow{w}, b$
P(y = 0) + P(y = 1) = 1

### Decision boundary
So pick a threshold to decide:
e.g.: Is $f_{\overrightarrow{w},b}(\overrightarrow{x}) \geq 0.5$?
Yes: $\hat{y} = 1$
No: $\hat{y} = 0$

#### When is $f \geq 0.5$?
$g(z) \geq 0.5$  
$z \geq 0$  
$\overrightarrow{w}·\overrightarrow{x} + b \geq 0$  
$\hat{y} = 1$

### Non-linear decision boundaries
+ e.g.: An oval: in oval $\hat{y} = 1$
$f_{\overrightarrow{w},b}(\overrightarrow{x}) = g(z) = g(w_1x_1^2+w_2x_2^2+b)$  
+ e.g.: A polynomial
$f_{\overrightarrow{w},b}(\overrightarrow{x}) = g(z) = g(w_1x_1+w_2x_2+w_3x_1^2+w_4x_1x_2+w_5x_2^2+b)$

## Cost function for logistic regression
Since squared error cost will be a non-convex function with logistic regression(has a lot of local minimum), it is not a good choice to use as cost function in GD.

### Definition:
+ **Loss** is a measure of the difference of a single example to its target value while the
+ **Cost** is a measure of the losses over the training set

### Logistic loss function
$$ L(f_{\overrightarrow{w},b}(\overrightarrow{x}^{(i)}),y^{(i)})=\begin{cases}
-log(f_{\overrightarrow{w},b}(\overrightarrow{x}^{(i)}))& \text{if $y^{(i)} = 1$}\\
-log(1-f_{\overrightarrow{w},b}(\overrightarrow{x}^{(i)}))& \text{if $y^{(i)} = 0$}
\end{cases}
$$  

If $y^{(i)} = 0$:  
$-log(1-f)$: when f = 0, it is 0. when f = 1, it is ∞
As $f_{\overrightarrow{w},b}(\overrightarrow{x}^{(i)}) \rightarrow 0$ then $loss \rightarrow 0$  
As $f_{\overrightarrow{w},b}(\overrightarrow{x}^{(i)}) \rightarrow 1$ then $loss \rightarrow \infin$  

### Overall cost function:
$J_{(w, b)} = \frac{1}{m}\sum^m_{i=1}(L(f_{\overrightarrow{w},b}(\overrightarrow{x}^{(i)}),y^{(i)}))$ -> convex

### Simplified cost function
Simplified loss function:
$L(f_{\overrightarrow{w},b}(\overrightarrow{x}^{(i)}),y^{(i)}) = -y^{(i)}log(f_{\overrightarrow{w},b}(\overrightarrow{x}^{(i)}))-(1-y^{(i)})log(1-f_{\overrightarrow{w},b}(\overrightarrow{x}^{(i)}))$

Cost:
$J_{(w, b)} = \frac{1}{m}\sum^m_{i=1}(L(f_{\overrightarrow{w},b}(\overrightarrow{x}^{(i)}),y^{(i)})) =\\
-\frac{1}{m}\sum^m_{i=1}[y^{(i)}log(f_{\overrightarrow{w},b}(\overrightarrow{x}^{(i)}))+(1-y^{(i)})log(1-f_{\overrightarrow{w},b}(\overrightarrow{x}^{(i)}))]$

## Gradient Descent implementation
Goal: Find $\overrightarrow{w},b$, given new $\overrightarrow{x}$, output $f_{\overrightarrow{w},b}(\overrightarrow{x}) = \frac{1}{q + e^{-(\overrightarrow{w}·\overrightarrow{x} + b)}}$

### Gradient Descent:
repeat{
    $w = w - \alpha \frac{d}{d\omega}J(w,b)$  
    $b = b - \alpha \frac{d}{d\omega}J(w,b)$  
}
Simplified:
repeat{
    $w_j = w_j-a[\frac{dJ(w,b)}{dw} = \frac{1}{m}\sum^{m-1}_{i=0}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)}]$
    $b = b- a[\frac{dJ(w,b)}{db} = \frac{1}{m}\sum^{m-1}_{i=0}(f_{w,b}(x^{(i)})-y^{(i)})]$
}

#### Same concepts with linear regression:
+ Monitor gradient descent (learning curve)
+ Vectorized implementation
+ Feature scaling

## The problem of overfitting
+ Underfit: does not fit the training set well/ high bias
+ Just right: fits the training set pretty well / generalization
+ Overfit: fits the training set extremely well / high variance

### Addressing overfitting
+ Collect more training examples
+ Select features to include/exclude
  + Con: useful features could be lost
+ Regularization: Reduce the size(weight) of parameters $w_j$

## Logistic Regression using Scikit-Learn
```
from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression()
lr_model.fit(X, y)
```
Accuracy on training set: `lr_model.score(X, y)`
Prediction on training set: `lr_model.predict(X)`