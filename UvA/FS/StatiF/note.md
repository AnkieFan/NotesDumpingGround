# Uncertainty in forensic science
## Terminology - Knowledge management:
+ *source* or *bulk form* evidence: Evidence such as this where the source is known and is of a bulk form
+ *receptor* or *transferred particle* evidence: They have been ‘received’ from somewhere by the suspect. 
+ *crime* evidence: Evidence found at the scene of the crime
+ *Locard’s principle*: Traces which are present on our clothes or our person are silent, sure and faithful witnesses of every action we do and of every meeting we have.
+ *transfer evidence* or *trace evidence*: evidence (e.g., blood or glass fragments) has been transferred from the criminal to the scene or vice versa
+ *control* / *known*: the material whose origin is known
+ *recovered* / *questioned*: the material whose origin is unknown
+ *known source*: The object or person on which traces have been recovered is defined as the *receptor* and the object or person that could be the source (or one of the sources) of the traces, and which is the origin of the material defined as *known material*
+ *scene-anchored* / *suspect-anchored*: two possibilities for the origin of the material which is taken to be known: the scene of the crime and the suspect
+ *crime sample*: a sample of material found at the scene of a crime
+ *suspect sample*: a sample of material found on or about a suspect

## Types of Data
+ *data*: A generic name for observations which are made on items of interest, such as bloodstains or refractive indices of glass
+ *qualitative data*:  These are not quantifiable. There is no numerical significance which may be attached to these.
+ *nominal data*: Qualitative data which have no natural ordering
+ *ordinal data*: Qualitative data to which a natural ordering may be attached. 
  + e.g.: ; level of trauma may be ordered as none, slight, mild, severe, very severe
+ *binary* / *dichotomous*: pos/neg
+ *quantitative data*: discrete or continuous

## Laws of probability
+ *First law of probability*: Probability can take any value between 0 and 1
+ *Second law of probability*: 
  + If R and S are independent: $P(R \cup S) = P(R)+P(S)$
  + If R and S are dependent: $$P(R \cup S) = P(R)+P(S) - P(R\cap S)$
+ *Third law of probability*: $P(R\cap S) = P(R)*P(S)$

## From exercises:
+ *stem-and-leaf display*
+ *ogive*: accumulation
+ *deciles*: 十分位
### Joint distribution
+ Covariance: 
  + $Cov_{x,y} = \frac{\sum(x_i- \overline{x})(y_i- \overline{y})}{N-1}$
  + $Cov(X,Y) = E((X-EX)(Y-EY)) = E(XY) - E(X)E(Y)$
+ Correlation: 
  + $Corr_{x,y} = \frac{Cov_{x,y}}{\sigma_X \sigma_Y}$, $\sigma$: standard variance
  + $Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$